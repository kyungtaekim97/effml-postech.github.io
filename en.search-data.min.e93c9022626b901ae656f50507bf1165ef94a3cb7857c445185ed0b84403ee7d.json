[{"id":0,"href":"/efficientml-postech.github.io/docs/spring24/","title":"Spring 2024","section":"Docs","content":" Blog-Post-Assignment # This is the Github page for the class, Efficient ML Systems (EECE695D-01). Students (will) upload a blog post reviewing the paper.\nGuideline for Students # First, make the folder named Paper Number_Paper Title_Member1_Member2 (e.g. 0_TACO_HagyeongLee). You can use initials of paper title. If there is no optimal initials, then you can skip the paper name (e.g. 0_HagyeongLee). Second, you put the markdown file and assests(e.g. figures) in the folder. Finally, if you verify that the markdown file is rendering successfully. Posts # Number Title Team Blog Post LINK Member 1, Member 2 0 (example) Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity Hagyeong Lee LINK 1 Is Bigger Edit Batch Size Always Better? \u0026ndash; An Empirical Study on Model Editing with Llama-3 Jin Hyun, Gyuhyun Jung 2 Spectrally Pruned Gaussian Fields with Neural Compensation Donggeon Lee, Chiho yoon 3 Unit Scaling: Out-of-the-Box Low-Precision Training SeongRok Moon, Changyoung Ju 4 Better \u0026amp; Faster Large Language Models via Multi-token Prediction Jinoh Cho, Seonghyeon Park 5 Lossless Self-Speculative Decoding via Double Early Exiting Nayoung Kwon, Jiwoong Im 6 XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference Hyundong Kim, Sangil Han 7 VeRA: Vector-based Random Matrix Adaptation Kyumin Cho, Sejin Park 8 Mixture of LoRA Experts Jegwang Ryu, Sangbeom Ha 9 MobileNetV4 \u0026ndash; Universal Models for the Mobile Ecosystem JoonSeok Kim, DongGyu Kim 10 Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length Hyunho Kook 11 Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention Younghyun Cho, Sangjun Lee 12 Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies Junkyeong Park, Harit Keawmuang 13 A Large-Scale Exploration of Î¼-Transfer Jeonghyun Choi, Minhye Choo 14 BinaryDM: Towards Accurate Binarization of Diffusion Model Junhyuk So, Juncheol Shin 15 Training LLMs over Neurally Compressed Text Seonghyun Park, Jiwoo Kim 16 Mixture-of-Depths: Dynamically allocating compute in transformer-based language models Minjae Park, Inkwan Hwang 17 QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs MyeongJi Yun, Jung Gyu Min 18 ViTAR: Vision Transformer with Any Resolution Jungwon Lee, Minsang Seok 19 LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning Sungbin Shin, Dongyeop Lee 20 Evolutionary Optimization of Model Merging Recipes Youngkil Song, Jaehyeon Park 21 A Unified Framework for Model Editing Jonghyun Chae, Donggeun An 22 Larimar: Large Language Models with Episodic Memory Control Sunggyu Jang, Hyeonwoo Park 23 Beyond Language Models: Byte Models are Digital World Simulators Dohun Kim, Yeongwoo Kim 24 LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression Seungjoo Shin, Sua Choi 25 Merging Text Transformer Models from Different Initializations Minwoo Kim, Kyungtae Kim Contact # If you have any questions, please feel free to contact TA (hagyeonglee@postech.ac.kr).\n"},{"id":1,"href":"/efficientml-postech.github.io/docs/spring24/posts/example_0_taco_hagyeonglee/","title":"[Example] 0 Taco Hagyeonglee","section":"Posts","content":" Example : Content # This paper propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, they avoid decoding based on text-guided generative models\u0026mdash;known for high generative diversity\u0026mdash;and effectively utilize the semantic information of text at a global level.\nExample : Using KaTeX for math equation # KaTeX shortcode let you render math typesetting in markdown document. See KaTeX\nHere is some inline example: \\(\\pi(x)\\) , rendered in the same line. And below is display example, having display: block \\[f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi\\] Text continues here.\n"}]